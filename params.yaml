activation: relu
ae_type: ae
epochs: 10
batch_size: 256
latent_dims:
- 2
- 4
- 8
- 16
- 32
- 64
- 128
- 256
encoder_layers: [512, 256]
decoder_layers: [256, 512]
learning_rate: 1e-3
weight_decay: 0.0
output_dir: "./outputs"
loss_fn: "mse"
num_workers: 2
device: auto
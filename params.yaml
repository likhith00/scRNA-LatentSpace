activation: relu
ae_type: ae
epochs: 10
batch_size: 256
latent_dims:
- 5
- 6
- 7
- 8
- 9
- 10
- 11
- 12
encoder_layers: [512, 256]
decoder_layers: [256, 512]
learning_rate: 1e-3
weight_decay: 0.0
output_dir: "./outputs"
loss_fn: "mse"
num_workers: 2
device: auto